{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nmt.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "kiUwiOITHTW4"
      ]
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "TjPTaRB4mpCd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Colab FAQ\n",
        "\n",
        "For some basic overview and features offered in Colab notebooks, check out: [Overview of Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n",
        "\n",
        "You need to use the colab GPU for this assignmentby selecting:\n",
        "\n",
        "> **Runtime**   →   **Change runtime type**   →   **Hardware Accelerator: GPU**"
      ]
    },
    {
      "metadata": {
        "id": "s9IS9B9-yUU5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup PyTorch\n",
        "All files are stored at /content/csc421/a3/ folder\n"
      ]
    },
    {
      "metadata": {
        "id": "Z-6MQhMOlHXD",
        "colab_type": "code",
        "outputId": "bbd05e0d-05f2-4c64-e1e8-88dad95231d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 569
        }
      },
      "cell_type": "code",
      "source": [
        "######################################################################\n",
        "# Setup python environment and change the current working directory\n",
        "######################################################################\n",
        "!pip install torch torchvision\n",
        "!pip install Pillow==4.0.0\n",
        "%mkdir -p /content/csc421/a3/\n",
        "%cd /content/csc421/a3"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python2.7/dist-packages (1.0.1.post2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python2.7/dist-packages (0.2.2.post3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python2.7/dist-packages (from torchvision) (1.14.6)\n",
            "Collecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0d/f3/421598450cb9503f4565d936860763b5af413a61009d87a5ab1e34139672/Pillow-5.4.1-cp27-cp27mu-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 10.0MB/s \n",
            "\u001b[31mfastai 0.7.0 has requirement torch<0.4, but you'll have torch 1.0.1.post2 which is incompatible.\u001b[0m\n",
            "\u001b[31mimgaug 0.2.8 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pillow\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.4.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting Pillow==4.0.0\n",
            "  Using cached https://files.pythonhosted.org/packages/89/99/0e3522a9764fe371bf9f7729404b1ef7d9c4fc49cbe5f1761c6e07812345/Pillow-4.0.0-cp27-cp27mu-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python2.7/dist-packages (from Pillow==4.0.0) (0.46)\n",
            "\u001b[31mfastai 0.7.0 has requirement torch<0.4, but you'll have torch 1.0.1.post2 which is incompatible.\u001b[0m\n",
            "\u001b[31mimgaug 0.2.8 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mtorchvision 0.2.2.post3 has requirement pillow>=4.1.1, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: Pillow\n",
            "  Found existing installation: Pillow 5.4.1\n",
            "    Uninstalling Pillow-5.4.1:\n",
            "      Successfully uninstalled Pillow-5.4.1\n",
            "Successfully installed Pillow-4.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9DaTdRNuUra7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Helper code"
      ]
    },
    {
      "metadata": {
        "id": "4BIpGwANoQOg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Utility functions"
      ]
    },
    {
      "metadata": {
        "id": "D-UJHBYZkh7f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pdb\n",
        "import argparse\n",
        "import pickle as pkl\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "import tarfile\n",
        "import pickle\n",
        "import sys\n",
        "\n",
        "\n",
        "def get_file(fname,\n",
        "             origin,\n",
        "             untar=False,\n",
        "             extract=False,\n",
        "             archive_format='auto',\n",
        "             cache_dir='data'):\n",
        "    datadir = os.path.join(cache_dir)\n",
        "    if not os.path.exists(datadir):\n",
        "        os.makedirs(datadir)\n",
        "\n",
        "    if untar:\n",
        "        untar_fpath = os.path.join(datadir, fname)\n",
        "        fpath = untar_fpath + '.tar.gz'\n",
        "    else:\n",
        "        fpath = os.path.join(datadir, fname)\n",
        "    \n",
        "    print(fpath)\n",
        "    if not os.path.exists(fpath):\n",
        "        print('Downloading data from', origin)\n",
        "\n",
        "        error_msg = 'URL fetch failure on {}: {} -- {}'\n",
        "        try:\n",
        "            try:\n",
        "                urlretrieve(origin, fpath)\n",
        "            except URLError as e:\n",
        "                raise Exception(error_msg.format(origin, e.errno, e.reason))\n",
        "            except HTTPError as e:\n",
        "                raise Exception(error_msg.format(origin, e.code, e.msg))\n",
        "        except (Exception, KeyboardInterrupt) as e:\n",
        "            if os.path.exists(fpath):\n",
        "                os.remove(fpath)\n",
        "            raise\n",
        "\n",
        "    if untar:\n",
        "        if not os.path.exists(untar_fpath):\n",
        "            print('Extracting file.')\n",
        "            with tarfile.open(fpath) as archive:\n",
        "                archive.extractall(datadir)\n",
        "        return untar_fpath\n",
        "\n",
        "    if extract:\n",
        "        _extract_archive(fpath, datadir, archive_format)\n",
        "\n",
        "    return fpath\n",
        "\n",
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "        \n",
        "def to_var(tensor, cuda):\n",
        "    \"\"\"Wraps a Tensor in a Variable, optionally placing it on the GPU.\n",
        "\n",
        "        Arguments:\n",
        "            tensor: A Tensor object.\n",
        "            cuda: A boolean flag indicating whether to use the GPU.\n",
        "\n",
        "        Returns:\n",
        "            A Variable object, on the GPU if cuda==True.\n",
        "    \"\"\"\n",
        "    if cuda:\n",
        "        return Variable(tensor.cuda())\n",
        "    else:\n",
        "        return Variable(tensor)\n",
        "\n",
        "\n",
        "def create_dir_if_not_exists(directory):\n",
        "    \"\"\"Creates a directory if it doesn't already exist.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "\n",
        "def save_loss_plot(train_losses, val_losses, opts):\n",
        "    \"\"\"Saves a plot of the training and validation loss curves.\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    plt.plot(range(len(train_losses)), train_losses)\n",
        "    plt.plot(range(len(val_losses)), val_losses)\n",
        "    plt.title('BS={}, nhid={}'.format(opts.batch_size, opts.hidden_size), fontsize=20)\n",
        "    plt.xlabel('Epochs', fontsize=16)\n",
        "    plt.ylabel('Loss', fontsize=16)\n",
        "    plt.xticks(fontsize=14)\n",
        "    plt.yticks(fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(opts.checkpoint_path, 'loss_plot.pdf'))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def checkpoint(encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Saves the current encoder and decoder models, along with idx_dict, which\n",
        "    contains the char_to_index and index_to_char mappings, and the start_token\n",
        "    and end_token values.\n",
        "    \"\"\"\n",
        "    with open(os.path.join(opts.checkpoint_path, 'encoder.pt'), 'wb') as f:\n",
        "        torch.save(encoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, 'decoder.pt'), 'wb') as f:\n",
        "        torch.save(decoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, 'idx_dict.pkl'), 'wb') as f:\n",
        "        pkl.dump(idx_dict, f)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pbvpn4MaV0I1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data loader"
      ]
    },
    {
      "metadata": {
        "id": "XVT4TNTOV3Eg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read_lines(filename):\n",
        "    \"\"\"Read a file and split it into lines.\n",
        "    \"\"\"\n",
        "    lines = open(filename).read().strip().lower().split('\\n')\n",
        "    return lines\n",
        "\n",
        "\n",
        "def read_pairs(filename):\n",
        "    \"\"\"Reads lines that consist of two words, separated by a space.\n",
        "\n",
        "    Returns:\n",
        "        source_words: A list of the first word in each line of the file.\n",
        "        target_words: A list of the second word in each line of the file.\n",
        "    \"\"\"\n",
        "    lines = read_lines(filename)\n",
        "    source_words, target_words = [], []\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            source, target = line.split()\n",
        "            source_words.append(source)\n",
        "            target_words.append(target)\n",
        "    return source_words, target_words\n",
        "\n",
        "\n",
        "def all_alpha_or_dash(s):\n",
        "    \"\"\"Helper function to check whether a string is alphabetic, allowing dashes '-'.\n",
        "    \"\"\"\n",
        "    return all(c.isalpha() or c == '-' for c in s)\n",
        "\n",
        "\n",
        "def filter_lines(lines):\n",
        "    \"\"\"Filters lines to consist of only alphabetic characters or dashes \"-\".\n",
        "    \"\"\"\n",
        "    return [line for line in lines if all_alpha_or_dash(line)]\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    \"\"\"Loads (English, Pig-Latin) word pairs, and creates mappings from characters to indexes.\n",
        "    \"\"\"\n",
        "\n",
        "    source_lines, target_lines = read_pairs('data/pig_latin_data.txt')\n",
        "\n",
        "    # Filter lines\n",
        "    source_lines = filter_lines(source_lines)\n",
        "    target_lines = filter_lines(target_lines)\n",
        "\n",
        "    all_characters = set(''.join(source_lines)) | set(''.join(target_lines))\n",
        "\n",
        "    # Create a dictionary mapping each character to a unique index\n",
        "    char_to_index = { char: index for (index, char) in enumerate(sorted(list(all_characters))) }\n",
        "\n",
        "    # Add start and end tokens to the dictionary\n",
        "    start_token = len(char_to_index)\n",
        "    end_token = len(char_to_index) + 1\n",
        "    char_to_index['SOS'] = start_token\n",
        "    char_to_index['EOS'] = end_token\n",
        "\n",
        "    # Create the inverse mapping, from indexes to characters (used to decode the model's predictions)\n",
        "    index_to_char = { index: char for (char, index) in char_to_index.items() }\n",
        "\n",
        "    # Store the final size of the vocabulary\n",
        "    vocab_size = len(char_to_index)\n",
        "\n",
        "    line_pairs = list(set(zip(source_lines, target_lines)))  # Python 3\n",
        "\n",
        "    idx_dict = { 'char_to_index': char_to_index,\n",
        "                 'index_to_char': index_to_char,\n",
        "                 'start_token': start_token,\n",
        "                 'end_token': end_token }\n",
        "\n",
        "    return line_pairs, vocab_size, idx_dict\n",
        "\n",
        "\n",
        "def create_dict(pairs):\n",
        "    \"\"\"Creates a mapping { (source_length, target_length): [list of (source, target) pairs]\n",
        "    This is used to make batches: each batch consists of two parallel tensors, one containing\n",
        "    all source indexes and the other containing all corresponding target indexes.\n",
        "    Within a batch, all the source words are the same length, and all the target words are\n",
        "    the same length.\n",
        "    \"\"\"\n",
        "    unique_pairs = list(set(pairs))  # Find all unique (source, target) pairs\n",
        "\n",
        "    d = defaultdict(list)\n",
        "    for (s,t) in unique_pairs:\n",
        "        d[(len(s), len(t))].append((s,t))\n",
        "\n",
        "    return d\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bRWfRdmVVjUl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training and evaluation code"
      ]
    },
    {
      "metadata": {
        "id": "wa5-onJhoSeM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def string_to_index_list(s, char_to_index, end_token):\n",
        "    \"\"\"Converts a sentence into a list of indexes (for each character).\n",
        "    \"\"\"\n",
        "    return [char_to_index[char] for char in s] + [end_token]  # Adds the end token to each index list\n",
        "\n",
        "\n",
        "def translate_sentence(sentence, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a sentence from English to Pig-Latin, by splitting the sentence into\n",
        "    words (whitespace-separated), running the encoder-decoder model to translate each\n",
        "    word independently, and then stitching the words back together with spaces between them.\n",
        "    \"\"\"\n",
        "    if idx_dict is None:\n",
        "      line_pairs, vocab_size, idx_dict = load_data()\n",
        "    return ' '.join([translate(word, encoder, decoder, idx_dict, opts) for word in sentence.split()])\n",
        "\n",
        "\n",
        "def translate(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a given string from English to Pig-Latin.\n",
        "    \"\"\"\n",
        "\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "    index_to_char = idx_dict['index_to_char']\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = ''\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_last_hidden = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_last_hidden\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "      ## slow decoding, recompute everything at each time\n",
        "      decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden)\n",
        "      generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "      ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "      ni = ni[-1] #latest output token\n",
        "\n",
        "      decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "      \n",
        "      if ni == end_token:\n",
        "          break\n",
        "      else:\n",
        "          gen_string = \"\".join(\n",
        "              [index_to_char[int(item)] \n",
        "               for item in generated_words.cpu().numpy().reshape(-1)])\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def visualize_attention(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Generates a heatmap to show where attention is focused in each decoder step.\n",
        "    \"\"\"\n",
        "    if idx_dict is None:\n",
        "      line_pairs, vocab_size, idx_dict = load_data()\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "    index_to_char = idx_dict['index_to_char']\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = ''\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_hidden = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    produced_end_token = False\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "      ## slow decoding, recompute everything at each time\n",
        "      decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden)\n",
        "      generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "      ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "      ni = ni[-1] #latest output token\n",
        "      \n",
        "      decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "      \n",
        "      if ni == end_token:\n",
        "          break\n",
        "      else:\n",
        "          gen_string = \"\".join(\n",
        "              [index_to_char[int(item)] \n",
        "               for item in generated_words.cpu().numpy().reshape(-1)])\n",
        "    \n",
        "    if isinstance(attention_weights, tuple):\n",
        "      ## transformer's attention mweights\n",
        "      attention_weights, self_attention_weights = attention_weights\n",
        "    \n",
        "    all_attention_weights = attention_weights.data.cpu().numpy()\n",
        "    \n",
        "    for i in range(len(all_attention_weights)):\n",
        "      attention_weights_matrix = all_attention_weights[i].squeeze()\n",
        "      fig = plt.figure()\n",
        "      ax = fig.add_subplot(111)\n",
        "      cax = ax.matshow(attention_weights_matrix, cmap='bone')\n",
        "      fig.colorbar(cax)\n",
        "\n",
        "      # Set up axes\n",
        "      ax.set_yticklabels([''] + list(input_string) + ['EOS'], rotation=90)\n",
        "      ax.set_xticklabels([''] + list(gen_string) + (['EOS'] if produced_end_token else []))\n",
        "\n",
        "      # Show label at every tick\n",
        "      ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "      ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "      # Add title\n",
        "      plt.xlabel('Attention weights to the source sentence in layer {}'.format(i+1))\n",
        "      plt.tight_layout()\n",
        "      plt.grid('off')\n",
        "      plt.show()\n",
        "      #plt.savefig(save)\n",
        "\n",
        "      #plt.close(fig)\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def compute_loss(data_dict, encoder, decoder, idx_dict, criterion, optimizer, opts):\n",
        "    \"\"\"Train/Evaluate the model on a dataset.\n",
        "\n",
        "    Arguments:\n",
        "        data_dict: The validation/test word pairs, organized by source and target lengths.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Train the weights if an optimizer is given. None if only evaluate the model. \n",
        "        opts: The command-line arguments.\n",
        "\n",
        "    Returns:\n",
        "        mean_loss: The average loss over all batches from data_dict.\n",
        "    \"\"\"\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "\n",
        "    losses = []\n",
        "    for key in data_dict:\n",
        "        input_strings, target_strings = zip(*data_dict[key])\n",
        "        input_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in input_strings]\n",
        "        target_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in target_strings]\n",
        "\n",
        "        num_tensors = len(input_tensors)\n",
        "        num_batches = int(np.ceil(num_tensors / float(opts.batch_size)))\n",
        "\n",
        "        for i in range(num_batches):\n",
        "\n",
        "            start = i * opts.batch_size\n",
        "            end = start + opts.batch_size\n",
        "\n",
        "            inputs = to_var(torch.stack(input_tensors[start:end]), opts.cuda)\n",
        "            targets = to_var(torch.stack(target_tensors[start:end]), opts.cuda)\n",
        "\n",
        "            # The batch size may be different in each epoch\n",
        "            BS = inputs.size(0)\n",
        "\n",
        "            encoder_annotations, encoder_hidden = encoder(inputs)\n",
        "\n",
        "            # The last hidden state of the encoder becomes the first hidden state of the decoder\n",
        "            decoder_hidden = encoder_hidden\n",
        "\n",
        "            start_vector = torch.ones(BS).long().unsqueeze(1) * start_token  # BS x 1 --> 16x1  CHECKED\n",
        "            decoder_input = to_var(start_vector, opts.cuda)  # BS x 1 --> 16x1  CHECKED\n",
        "\n",
        "            loss = 0.0\n",
        "\n",
        "            seq_len = targets.size(1)  # Gets seq_len from BS x seq_len\n",
        "\n",
        "            decoder_inputs = torch.cat([decoder_input, targets[:, 0:-1]], dim=1)  # Gets decoder inputs by shifting the targets to the right \n",
        "            \n",
        "            decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, encoder_hidden)\n",
        "            decoder_outputs_flatten = decoder_outputs.view(-1, decoder_outputs.size(2))\n",
        "            targets_flatten = targets.view(-1)\n",
        "            loss = criterion(decoder_outputs_flatten, targets_flatten)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            ## training if an optimizer is provided\n",
        "            if optimizer:\n",
        "              # Zero gradients\n",
        "              optimizer.zero_grad()\n",
        "              # Compute gradients\n",
        "              loss.backward()\n",
        "              # Update the parameters of the encoder and decoder\n",
        "              optimizer.step()\n",
        "              \n",
        "    mean_loss = np.mean(losses)\n",
        "    return mean_loss\n",
        "\n",
        "  \n",
        "\n",
        "def training_loop(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts):\n",
        "    \"\"\"Runs the main training loop; evaluates the model on the val set every epoch.\n",
        "        * Prints training and val loss each epoch.\n",
        "        * Prints qualitative translation results each epoch using TEST_SENTENCE\n",
        "        * Saves an attention map for TEST_WORD_ATTN each epoch\n",
        "\n",
        "    Arguments:\n",
        "        train_dict: The training word pairs, organized by source and target lengths.\n",
        "        val_dict: The validation word pairs, organized by source and target lengths.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Implements a step rule to update the parameters of the encoder and decoder.\n",
        "        opts: The command-line arguments.\n",
        "    \"\"\"\n",
        "\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "\n",
        "    loss_log = open(os.path.join(opts.checkpoint_path, 'loss_log.txt'), 'w')\n",
        "\n",
        "    best_val_loss = 1e6\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(opts.nepochs):\n",
        "\n",
        "        optimizer.param_groups[0]['lr'] *= opts.lr_decay\n",
        "        \n",
        "        train_loss = compute_loss(train_dict, encoder, decoder, idx_dict, criterion, optimizer, opts)\n",
        "        val_loss = compute_loss(val_dict, encoder, decoder, idx_dict, criterion, None, opts)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            checkpoint(encoder, decoder, idx_dict, opts)\n",
        "\n",
        "        gen_string = translate_sentence(TEST_SENTENCE, encoder, decoder, idx_dict, opts)\n",
        "        print(\"Epoch: {:3d} | Train loss: {:.3f} | Val loss: {:.3f} | Gen: {:20s}\".format(epoch, train_loss, val_loss, gen_string))\n",
        "\n",
        "        loss_log.write('{} {} {}\\n'.format(epoch, train_loss, val_loss))\n",
        "        loss_log.flush()\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        save_loss_plot(train_losses, val_losses, opts)\n",
        "\n",
        "\n",
        "def print_data_stats(line_pairs, vocab_size, idx_dict):\n",
        "    \"\"\"Prints example word pairs, the number of data points, and the vocabulary.\n",
        "    \"\"\"\n",
        "    print('=' * 80)\n",
        "    print('Data Stats'.center(80))\n",
        "    print('-' * 80)\n",
        "    for pair in line_pairs[:5]:\n",
        "        print(pair)\n",
        "    print('Num unique word pairs: {}'.format(len(line_pairs)))\n",
        "    print('Vocabulary: {}'.format(idx_dict['char_to_index'].keys()))\n",
        "    print('Vocab size: {}'.format(vocab_size))\n",
        "    print('=' * 80)\n",
        "\n",
        "\n",
        "def train(opts):\n",
        "    line_pairs, vocab_size, idx_dict = load_data()\n",
        "    print_data_stats(line_pairs, vocab_size, idx_dict)\n",
        "\n",
        "    # Split the line pairs into an 80% train and 20% val split\n",
        "    num_lines = len(line_pairs)\n",
        "    num_train = int(0.8 * num_lines)\n",
        "    train_pairs, val_pairs = line_pairs[:num_train], line_pairs[num_train:]\n",
        "\n",
        "    # Group the data by the lengths of the source and target words, to form batches\n",
        "    train_dict = create_dict(train_pairs)\n",
        "    val_dict = create_dict(val_pairs)\n",
        "\n",
        "    ##########################################################################\n",
        "    ### Setup: Create Encoder, Decoder, Learning Criterion, and Optimizers ###\n",
        "    ##########################################################################\n",
        "    encoder = GRUEncoder(vocab_size=vocab_size, \n",
        "                         hidden_size=opts.hidden_size, \n",
        "                         opts=opts)\n",
        "\n",
        "    if opts.decoder_type == 'rnn':\n",
        "        decoder = RNNDecoder(vocab_size=vocab_size, \n",
        "                             hidden_size=opts.hidden_size)\n",
        "    elif opts.decoder_type == 'rnn_attention':\n",
        "        decoder = RNNAttentionDecoder(vocab_size=vocab_size, \n",
        "                                      hidden_size=opts.hidden_size, \n",
        "                                      attention_type=opts.attention_type)\n",
        "    elif opts.decoder_type == 'transformer':\n",
        "        decoder = TransformerDecoder(vocab_size=vocab_size, \n",
        "                                     hidden_size=opts.hidden_size, \n",
        "                                     num_layers=opts.num_transformer_layers)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    #### setup checkpoint path\n",
        "    model_name = 'h{}-bs{}-{}'.format(opts.hidden_size, \n",
        "                                      opts.batch_size, \n",
        "                                      opts.decoder_type)\n",
        "    opts.checkpoint_path = model_name\n",
        "    create_dir_if_not_exists(opts.checkpoint_path)\n",
        "    ####\n",
        "\n",
        "    if opts.cuda:\n",
        "        encoder.cuda()\n",
        "        decoder.cuda()\n",
        "        print(\"Moved models to GPU!\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=opts.learning_rate)\n",
        "\n",
        "    try:\n",
        "        training_loop(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts)\n",
        "    except KeyboardInterrupt:\n",
        "        print('Exiting early from training.')\n",
        "        return encoder, decoder\n",
        "      \n",
        "    return encoder, decoder\n",
        "\n",
        "\n",
        "def print_opts(opts):\n",
        "    \"\"\"Prints the values of all command-line arguments.\n",
        "    \"\"\"\n",
        "    print('=' * 80)\n",
        "    print('Opts'.center(80))\n",
        "    print('-' * 80)\n",
        "    for key in opts.__dict__:\n",
        "        print('{:>30}: {:<30}'.format(key, opts.__dict__[key]).center(80))\n",
        "    print('=' * 80)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bXNsLNkOn38w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Your code for NMT models"
      ]
    },
    {
      "metadata": {
        "id": "_BAfi_8yWB3y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## GRU cell"
      ]
    },
    {
      "metadata": {
        "id": "9ztmyA5Ro67o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MyGRUCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(MyGRUCell, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        self.w_i = nn.Linear(self.input_size, self.hidden_size)\n",
        "        self.w_h = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        \n",
        "\n",
        "\n",
        "    def forward(self, x, h_prev):\n",
        "        \"\"\"Forward pass of the GRU computation for one time step.\n",
        "\n",
        "        Arguments\n",
        "            x: batch_size x input_size\n",
        "            h_prev: batch_size x hidden_size\n",
        "\n",
        "        Returns:\n",
        "            h_new: batch_size x hidden_size\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        z = nn.functional.sigmoid(self.w_i(x)+self.w_h(h_prev))\n",
        "        r = nn.functional.sigmoid(self.w_i(x)+self.w_h(h_prev))\n",
        "        g = nn.functional.tanh(self.w_i(x) + r*(self.w_h(h_prev)))\n",
        "        h_new = (1-z)*g+z*h_prev\n",
        "        return h_new"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-JBVFLEZWNC1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### GRU encoder / decoder"
      ]
    },
    {
      "metadata": {
        "id": "xaDt7XDmWRzC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class GRUEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, opts):\n",
        "        super(GRUEncoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.opts = opts\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.gru = nn.GRUCell(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass of the encoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
        "\n",
        "        Returns:\n",
        "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden: The final hidden state of the encoder, for each sequence in a batch. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len = inputs.size()\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "\n",
        "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "        annotations = []\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            x = encoded[:,i,:]  # Get the current time step, across the whole batch\n",
        "            hidden = self.gru(x, hidden)\n",
        "            annotations.append(hidden)\n",
        "\n",
        "        annotations = torch.stack(annotations, dim=1)\n",
        "        return annotations, hidden\n",
        "\n",
        "    def init_hidden(self, bs):\n",
        "        \"\"\"Creates a tensor of zeros to represent the initial hidden states\n",
        "        of a batch of sequences.\n",
        "\n",
        "        Arguments:\n",
        "            bs: The batch size for the initial hidden state.\n",
        "\n",
        "        Returns:\n",
        "            hidden: An initial hidden state of all zeros. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "        return to_var(torch.zeros(bs, self.hidden_size), self.opts.cuda)\n",
        "\n",
        "\n",
        "class RNNDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(RNNDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rnn = nn.GRUCell(input_size=hidden_size, hidden_size=hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the non-attentional decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch. (batch_size x seq_len)\n",
        "            annotations: This is not used here. It just maintains consistency with the\n",
        "                    interface used by the AttentionDecoder class.\n",
        "            hidden_init: The hidden states from the last step of encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            None\n",
        "        \"\"\"        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
        "\n",
        "        hiddens = []\n",
        "        h_prev = hidden_init\n",
        "        for i in range(seq_len):\n",
        "            x = embed[:,i,:]  # Get the current time step input tokens, across the whole batch\n",
        "            h_prev = self.rnn(x, h_prev)  # batch_size x hidden_size\n",
        "            hiddens.append(h_prev)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1) # batch_size x seq_len x hidden_size\n",
        "        \n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, None      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tWe0RO5FWajD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Attention"
      ]
    },
    {
      "metadata": {
        "id": "9GUK5A7CWhV8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class AdditiveAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(AdditiveAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # A two layer fully-connected network\n",
        "        # hidden_size*2 --> hidden_size, ReLU, hidden_size --> 1\n",
        "        self.attention_network = nn.Sequential(\n",
        "                                    nn.Linear(hidden_size*2, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Linear(hidden_size, 1)\n",
        "                                 )\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the additive attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state. (batch_size x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x 1 x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The attention_weights must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        # batch_size = ...\n",
        "        # expanded_queries = ...\n",
        "        # concat_inputs = ...\n",
        "        # unnormalized_attention = ...\n",
        "        # attention_weights = ...\n",
        "        # context = ...\n",
        "        # return context, attention_weights\n",
        "        batch_size, seq_len, hidden_size = keys.size()\n",
        "        expanded_queries = queries.unsqueeze(1).expand_as(keys)\n",
        "        concat_inputs = torch.cat((keys, expanded_queries), 2)\n",
        "        unnormalized_attention = self.attention_network(concat_inputs.view(batch_size * seq_len, 2 * hidden_size)).view(batch_size, seq_len, 1) \n",
        "        attention_weights = self.softmax(unnormalized_attention)\n",
        "        context = torch.bmm(attention_weights.view(batch_size, 1, seq_len), values)\n",
        "        return context, attention_weights\n",
        "      \n",
        "\n",
        "class ScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(ScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        # batch_size = ...\n",
        "        # q = ...\n",
        "        # k = ...\n",
        "        # v = ...\n",
        "        # unnormalized_attention = ...\n",
        "        # attention_weights = ...\n",
        "        # context = ...\n",
        "        # return context, attention_weights\n",
        "        batch_size, seq_len, hidden_size = keys.size()\n",
        "        \n",
        "        q = self.Q(queries)\n",
        "        k = self.K(keys)\n",
        "        v = self.V(values)\n",
        "        if len(q.size()) == 2:\n",
        "          q = self.Q(queries).unsqueeze(1)\n",
        "        unnormalized_attention = torch.bmm(k, torch.transpose(q, 1, 2)) * self.scaling_factor\n",
        "        attention_weights = self.softmax(unnormalized_attention)\n",
        "        context = torch.bmm(torch.transpose(attention_weights, 1, 2), v)\n",
        "        return context, attention_weights\n",
        "      \n",
        "      \n",
        "class CausalScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(CausalScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.neg_inf = torch.tensor(-1e7)\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        # batch_size = ...\n",
        "        # q = ...\n",
        "        # k = ...\n",
        "        # v = ...\n",
        "        # unnormalized_attention = ...\n",
        "        # mask = ...\n",
        "        # attention_weights = ...\n",
        "        # context = ...\n",
        "        # return context, attention_weights\n",
        "        batch_size, seq_len, hidden_size = keys.size()\n",
        "        q = self.Q(queries)\n",
        "        k = self.K(keys)\n",
        "        v = self.V(values)\n",
        "        if len(q.size()) == 2:\n",
        "          q = self.Q(queries).unsqueeze(1)\n",
        "        s = q.size()[1]\n",
        "        \n",
        "        \n",
        "        unnormalized_attention = torch.bmm(k, torch.transpose(q, 1, 2)) * self.scaling_factor\n",
        "        ones = torch.ones((seq_len, s), dtype=torch.uint8)\n",
        "        mask = torch.tril(ones, diagonal=s-seq_len).cuda()\n",
        "        unnormalized_attention = unnormalized_attention+mask*self.neg_inf\n",
        "        attention_weights = self.softmax(unnormalized_attention)\n",
        "        context = torch.bmm(torch.transpose(attention_weights, 1, 2), v)\n",
        "        return context, attention_weights\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C4UIC9hU_NY3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pemjZo2XWtRt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Attention decoder"
      ]
    },
    {
      "metadata": {
        "id": "PfjF0Z-PWwPv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RNNAttentionDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, attention_type='scaled_dot'):\n",
        "        super(RNNAttentionDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "\n",
        "        self.rnn = MyGRUCell(input_size=hidden_size*2, hidden_size=hidden_size)\n",
        "        if attention_type == 'additive':\n",
        "          self.attention = AdditiveAttention(hidden_size=hidden_size)\n",
        "        elif attention_type == 'scaled_dot':\n",
        "          self.attention = ScaledDotAttention(hidden_size=hidden_size)\n",
        "        \n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        \n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: The final hidden states from the encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
        "\n",
        "        hiddens = []\n",
        "        attentions = []\n",
        "        h_prev = hidden_init\n",
        "        for i in range(seq_len):\n",
        "            # ------------\n",
        "            # FILL THIS IN\n",
        "            # ------------\n",
        "            # embed_current = ...\n",
        "            # context, attention_weights = ...\n",
        "            # embed_and_context = ...\n",
        "            # h_prev = ...\n",
        "            embed_current = embed[:,i,:].squeeze(1)\n",
        "            context, attention_weights = self.attention.forward(h_prev, annotations, annotations)\n",
        "            embed_and_context = torch.cat((context.squeeze(1), embed_current), 1) \n",
        "            h_prev = self.rnn.forward(embed_and_context, h_prev)\n",
        "            \n",
        "            hiddens.append(h_prev)\n",
        "            attentions.append(attention_weights)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1) # batch_size x seq_len x hidden_size\n",
        "        attentions = torch.cat(attentions, dim=2) # batch_size x seq_len x seq_len\n",
        "        \n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, attentions\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N8JpcwTRW5cw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Transformer decoder"
      ]
    },
    {
      "metadata": {
        "id": "V5vJPku1W7sz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)        \n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.self_attentions = nn.ModuleList([CausalScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.encoder_attentions = nn.ModuleList([ScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.attention_mlps = nn.ModuleList([nn.Sequential(\n",
        "                                    nn.Linear(hidden_size, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        \n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: Not used in the transformer decoder\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
        "\n",
        "        encoder_attention_weights_list = []\n",
        "        self_attention_weights_list = []\n",
        "        contexts = embed\n",
        "        for i in range(self.num_layers):\n",
        "          # ------------\n",
        "          # FILL THIS IN\n",
        "          # ------------\n",
        "          # new_contexts, self_attention_weights = ...\n",
        "          # residual_contexts = ...\n",
        "          # new_contexts, encoder_attention_weights = ...\n",
        "          # residual_contexts = ...\n",
        "          # new_contexts = ...\n",
        "          # contexts = ...\n",
        "          new_contexts, self_attention_weights = self.self_attentions[i](contexts, contexts, contexts)\n",
        "          residual_contexts = contexts + new_contexts\n",
        "          new_contexts, encoder_attention_weights = self.encoder_attentions[i](residual_contexts, annotations, annotations)\n",
        "          residual_contexts = residual_contexts + new_contexts\n",
        "          new_contexts = self.attention_mlps[i](residual_contexts)\n",
        "          contexts = residual_contexts + new_contexts + contexts\n",
        "          \n",
        "          encoder_attention_weights_list.append(encoder_attention_weights)\n",
        "          self_attention_weights_list.append(self_attention_weights)\n",
        "          \n",
        "        output = self.out(contexts)\n",
        "        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n",
        "        self_attention_weights = torch.stack(self_attention_weights_list)\n",
        "        \n",
        "        return output, (encoder_attention_weights, self_attention_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XuNFd6LNo0-o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training\n"
      ]
    },
    {
      "metadata": {
        "id": "kiUwiOITHTW4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download dataset"
      ]
    },
    {
      "metadata": {
        "id": "xwcFjsEpHRbI",
        "colab_type": "code",
        "outputId": "ed782dd3-1f5a-47f4-f695-40957f2fde02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "######################################################################\n",
        "# Download Translation datasets\n",
        "######################################################################\n",
        "data_fpath = get_file(fname='pig_latin_data.txt', \n",
        "                         origin='http://www.cs.toronto.edu/~jba/pig_latin_data.txt', \n",
        "                         untar=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data/pig_latin_data.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hmQmyJDSRFKR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## RNN decoder"
      ]
    },
    {
      "metadata": {
        "id": "0LKaRF1jwhH7",
        "colab_type": "code",
        "outputId": "0300cf41-54f1-406d-8fdf-ed11caed8ac9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2187
        }
      },
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005, \n",
        "              'lr_decay':0.99,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':20, \n",
        "              'decoder_type': 'rnn', # options: rnn / rnn_attention / transformer\n",
        "              'attention_type': '',  # options: additive / scaled_dot\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "rnn_encoder, rnn_decoder = train(args)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_encoder, rnn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                            hidden_size: 20                                     \n",
            "                          learning_rate: 0.005                                  \n",
            "                             batch_size: 64                                     \n",
            "                                nepochs: 100                                    \n",
            "                                   cuda: 1                                      \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                           decoder_type: rnn                                    \n",
            "                               lr_decay: 0.99                                   \n",
            "                         attention_type:                                        \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('payment', 'aymentpay')\n",
            "('ordination', 'ordinationway')\n",
            "('amends', 'amendsway')\n",
            "('principally', 'incipallypray')\n",
            "('anybody', 'anybodyway')\n",
            "Num unique word pairs: 6387\n",
            "Vocabulary: ['EOS', '-', 'SOS', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z']\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.362 | Val loss: 2.018 | Gen: eray eray eray eray eray\n",
            "Epoch:   1 | Train loss: 1.955 | Val loss: 1.866 | Gen: eray atedway ongray ay oongsay\n",
            "Epoch:   2 | Train loss: 1.798 | Val loss: 1.780 | Gen: esedway atedway ongay ingssay oungsay\n",
            "Epoch:   3 | Train loss: 1.693 | Val loss: 1.704 | Gen: estedway ancedway ongay ingsay oungthay\n",
            "Epoch:   4 | Train loss: 1.611 | Val loss: 1.644 | Gen: ededway alingedway onsay-ongshay ingedway oungtay\n",
            "Epoch:   5 | Train loss: 1.558 | Val loss: 1.593 | Gen: ededway alledway onsingedway indway oungay-ongshay\n",
            "Epoch:   6 | Train loss: 1.490 | Val loss: 1.562 | Gen: edstay aringay oningortay indway oungray-onghay\n",
            "Epoch:   7 | Train loss: 1.439 | Val loss: 1.525 | Gen: edsay aingedway oningorthay inday oungray-ongshay\n",
            "Epoch:   8 | Train loss: 1.419 | Val loss: 1.518 | Gen: estedway ainssay oningorthay indway oungray-ongshay\n",
            "Epoch:   9 | Train loss: 1.380 | Val loss: 1.477 | Gen: estedway aingstay oningorthay indway oungay-onsightray\n",
            "Epoch:  10 | Train loss: 1.348 | Val loss: 1.458 | Gen: estray ainsssay oningorterway inday ountedway\n",
            "Epoch:  11 | Train loss: 1.319 | Val loss: 1.434 | Gen: estray ainsssay oningorterway inday ountedway\n",
            "Epoch:  12 | Train loss: 1.285 | Val loss: 1.429 | Gen: estshay aingstay oningortedway idway oulway-awlay\n",
            "Epoch:  13 | Train loss: 1.263 | Val loss: 1.402 | Gen: estray ainssway oningortedway idway ourtedway\n",
            "Epoch:  14 | Train loss: 1.242 | Val loss: 1.398 | Gen: estray aingway oningortedway idway ourtedway\n",
            "Epoch:  15 | Train loss: 1.220 | Val loss: 1.395 | Gen: epray aingway oningoringday idway oulingway\n",
            "Epoch:  16 | Train loss: 1.254 | Val loss: 1.403 | Gen: epray ainssway oningertray idway orfingcay\n",
            "Epoch:  17 | Train loss: 1.191 | Val loss: 1.377 | Gen: epray ainssway oningertray iway ountedway\n",
            "Epoch:  18 | Train loss: 1.169 | Val loss: 1.371 | Gen: epray ainssway oningortway iway oustay-ayday\n",
            "Epoch:  19 | Train loss: 1.159 | Val loss: 1.355 | Gen: epray aistway oningertray iway orfingdray\n",
            "Epoch:  20 | Train loss: 1.147 | Val loss: 1.351 | Gen: epray ainssway oningdonsway iway oursay\n",
            "Epoch:  21 | Train loss: 1.131 | Val loss: 1.342 | Gen: emay ainssay oningdonsway iway oustay-ighay\n",
            "Epoch:  22 | Train loss: 1.119 | Val loss: 1.349 | Gen: emay aistway oningeray iway oustedway\n",
            "Epoch:  23 | Train loss: 1.128 | Val loss: 1.350 | Gen: emay aistway oningondedway iway oussshay\n",
            "Epoch:  24 | Train loss: 1.113 | Val loss: 1.312 | Gen: emay aistway oningeray iway orfuringway\n",
            "Epoch:  25 | Train loss: 1.081 | Val loss: 1.305 | Gen: epay aistway oningerway iway oulingway\n",
            "Epoch:  26 | Train loss: 1.065 | Val loss: 1.291 | Gen: emay aistway oningdonday iway orfuringway\n",
            "Epoch:  27 | Train loss: 1.071 | Val loss: 1.299 | Gen: epay aistway oningdonday iway oulingway\n",
            "Epoch:  28 | Train loss: 1.057 | Val loss: 1.291 | Gen: etay aistway oningdonday iway oussingday\n",
            "Epoch:  29 | Train loss: 1.038 | Val loss: 1.280 | Gen: eattray aistway onsingdorway iway orfuringway\n",
            "Epoch:  30 | Train loss: 1.026 | Val loss: 1.286 | Gen: earay aistway onsingdoussway iway orfuringway\n",
            "Epoch:  31 | Train loss: 1.021 | Val loss: 1.275 | Gen: earway aistway onsingdonday iway orfuringway\n",
            "Epoch:  32 | Train loss: 1.030 | Val loss: 1.314 | Gen: epray aistway onsingdorway iway orfuringway\n",
            "Epoch:  33 | Train loss: 1.023 | Val loss: 1.275 | Gen: earway aistway onsingdonday iway orkingray\n",
            "Epoch:  34 | Train loss: 1.012 | Val loss: 1.271 | Gen: earway aisssay onsingdorway iway orkingray\n",
            "Epoch:  35 | Train loss: 0.994 | Val loss: 1.252 | Gen: eattray aistway onsingdonday iway orkingray\n",
            "Epoch:  36 | Train loss: 0.991 | Val loss: 1.266 | Gen: eattray aissway onsingdonday iway orkingray\n",
            "Epoch:  37 | Train loss: 1.003 | Val loss: 1.284 | Gen: eay aissway onsingdomay idway orfuringway\n",
            "Epoch:  38 | Train loss: 1.005 | Val loss: 1.285 | Gen: empay aisssay onsingdorway idway oulingway\n",
            "Epoch:  39 | Train loss: 0.991 | Val loss: 1.266 | Gen: eattstay aishway onsingdorfay isfay orkingray\n",
            "Epoch:  40 | Train loss: 0.972 | Val loss: 1.233 | Gen: earway aissway onsingdorsay isway orkingray\n",
            "Epoch:  41 | Train loss: 0.958 | Val loss: 1.244 | Gen: eattay aistway onsingdorway isway orkingray\n",
            "Epoch:  42 | Train loss: 0.959 | Val loss: 1.220 | Gen: earway aistway onsingdorway isfay oulingway\n",
            "Epoch:  43 | Train loss: 0.942 | Val loss: 1.236 | Gen: eattray aistway onsingdorway isfay orkingray\n",
            "Epoch:  44 | Train loss: 0.943 | Val loss: 1.214 | Gen: earway aistway onssingdray issway orkingray\n",
            "Epoch:  45 | Train loss: 0.967 | Val loss: 1.224 | Gen: eattray aistway onsingdomfay isway orkingray\n",
            "Epoch:  46 | Train loss: 0.937 | Val loss: 1.210 | Gen: eattray aistway onsingdorway issway orkingray\n",
            "Epoch:  47 | Train loss: 0.920 | Val loss: 1.213 | Gen: eattay aistway onsingdomway isway orkingray\n",
            "Epoch:  48 | Train loss: 0.931 | Val loss: 1.208 | Gen: eattray aistway onsingdorway issway orkingstay\n",
            "Epoch:  49 | Train loss: 0.907 | Val loss: 1.204 | Gen: eattray aishway onssingdray isway orkingstay\n",
            "Epoch:  50 | Train loss: 0.907 | Val loss: 1.201 | Gen: eattray aistway onssingdshay issway orkingstay\n",
            "Epoch:  51 | Train loss: 0.907 | Val loss: 1.221 | Gen: eattray aishway onsingigedway issway orkingstay\n",
            "Epoch:  52 | Train loss: 0.902 | Val loss: 1.201 | Gen: eattray aishway onssingshay issway orkingway\n",
            "Epoch:  53 | Train loss: 0.896 | Val loss: 1.197 | Gen: eattray aishtay onssingigchay issway orkingway\n",
            "Epoch:  54 | Train loss: 0.892 | Val loss: 1.190 | Gen: etay aishway onssingdshay issay orkingway\n",
            "Epoch:  55 | Train loss: 0.882 | Val loss: 1.188 | Gen: etay aistway onssingdray issay orkingstay\n",
            "Epoch:  56 | Train loss: 0.878 | Val loss: 1.179 | Gen: etay aistway onssingshay isway orkingstay\n",
            "Epoch:  57 | Train loss: 0.882 | Val loss: 1.187 | Gen: etay aistway onssingigchay issway orkingstay\n",
            "Epoch:  58 | Train loss: 0.896 | Val loss: 1.198 | Gen: eway aishtay onssingshay idsay orkingway\n",
            "Epoch:  59 | Train loss: 0.882 | Val loss: 1.188 | Gen: etay aistway onssingshay isway orkingstay\n",
            "Epoch:  60 | Train loss: 0.888 | Val loss: 1.174 | Gen: etay aistway onssingshay issway orkingway\n",
            "Epoch:  61 | Train loss: 0.861 | Val loss: 1.165 | Gen: etay aistway onssingshay issay orkingway\n",
            "Epoch:  62 | Train loss: 0.844 | Val loss: 1.169 | Gen: ethay aistway onssingshay issay orkingway\n",
            "Epoch:  63 | Train loss: 0.837 | Val loss: 1.162 | Gen: etay aistway onssingshay isway orkingstay\n",
            "Epoch:  64 | Train loss: 0.835 | Val loss: 1.176 | Gen: etay aithay onssingshay issay orkingstay\n",
            "Epoch:  65 | Train loss: 0.831 | Val loss: 1.165 | Gen: etay aithay onssingshay isway orkingstay\n",
            "Epoch:  66 | Train loss: 0.842 | Val loss: 1.172 | Gen: eway aistybay onssingshay issay orkingstay\n",
            "Epoch:  67 | Train loss: 0.892 | Val loss: 1.173 | Gen: eway aithay onssingdshay isway orkingstay\n",
            "Epoch:  68 | Train loss: 0.852 | Val loss: 1.166 | Gen: etay aishtay onssingshay issay orkingway\n",
            "Epoch:  69 | Train loss: 0.833 | Val loss: 1.148 | Gen: ethay aishtay onssingshay issay orkingway\n",
            "Epoch:  70 | Train loss: 0.827 | Val loss: 1.144 | Gen: ethay aishtay onssingdshay isway orkingway\n",
            "Epoch:  71 | Train loss: 0.817 | Val loss: 1.144 | Gen: ethay aithay onssingshay issay orkingstay\n",
            "Epoch:  72 | Train loss: 0.808 | Val loss: 1.142 | Gen: ethay aithway onssingshay issay orkingstay\n",
            "Epoch:  73 | Train loss: 0.810 | Val loss: 1.144 | Gen: ethay aithay onssingshay isway orkingway\n",
            "Epoch:  74 | Train loss: 0.810 | Val loss: 1.172 | Gen: etay aishtay onssingshay issay orkingstay\n",
            "Epoch:  75 | Train loss: 0.822 | Val loss: 1.138 | Gen: ethay aishtay onssingdshay isway orkingstay\n",
            "Epoch:  76 | Train loss: 0.895 | Val loss: 1.224 | Gen: ethay aithay onsssonsedway idway orkingway\n",
            "Epoch:  77 | Train loss: 0.872 | Val loss: 1.140 | Gen: ethay aishway onssingshay idsay orkingstay\n",
            "Epoch:  78 | Train loss: 0.823 | Val loss: 1.139 | Gen: ethay aishway onssingshay idsay orkingway\n",
            "Epoch:  79 | Train loss: 0.811 | Val loss: 1.154 | Gen: ethay aishway onssingshay issay orkingway\n",
            "Epoch:  80 | Train loss: 0.808 | Val loss: 1.118 | Gen: ethay aishway onssishstay idsay orkingway\n",
            "Epoch:  81 | Train loss: 0.789 | Val loss: 1.115 | Gen: ethay aishtay onssingshay issay orkingway\n",
            "Epoch:  82 | Train loss: 0.785 | Val loss: 1.107 | Gen: ethay aishtay onsssonsfay issay orkingway\n",
            "Epoch:  83 | Train loss: 0.784 | Val loss: 1.113 | Gen: ethay aithay onssingshay issay orkingway\n",
            "Epoch:  84 | Train loss: 0.783 | Val loss: 1.118 | Gen: ethay aishtay onsssingfray issay orkingstay\n",
            "Epoch:  85 | Train loss: 0.789 | Val loss: 1.108 | Gen: ethay aishtay onssingshay issay orkingway\n",
            "Epoch:  86 | Train loss: 0.779 | Val loss: 1.124 | Gen: ethay airyday onssingdray isway orkingway\n",
            "Epoch:  87 | Train loss: 0.778 | Val loss: 1.097 | Gen: ethay aithay onssingshay issay orkingstay\n",
            "Epoch:  88 | Train loss: 0.766 | Val loss: 1.102 | Gen: ethay airysay onssingshay isway orkingway\n",
            "Epoch:  89 | Train loss: 0.769 | Val loss: 1.101 | Gen: ethay airysay onsssingdray issay orkingstay\n",
            "Epoch:  90 | Train loss: 0.765 | Val loss: 1.101 | Gen: ethay airyway onssingdray idsay orkingway\n",
            "Epoch:  91 | Train loss: 0.762 | Val loss: 1.131 | Gen: ethay airysay onssishssay issay orkingway\n",
            "Epoch:  92 | Train loss: 0.767 | Val loss: 1.101 | Gen: ethay airysay onssingshay idsay orkingway\n",
            "Epoch:  93 | Train loss: 0.770 | Val loss: 1.093 | Gen: ethay airysay onsssshay-inway isway orkingway\n",
            "Epoch:  94 | Train loss: 0.783 | Val loss: 1.098 | Gen: ethay airysay onssingshay idsay orksay-imethay\n",
            "Epoch:  95 | Train loss: 0.774 | Val loss: 1.100 | Gen: ethay airysay onsssingfray isway orksay-ibleway\n",
            "Epoch:  96 | Train loss: 0.757 | Val loss: 1.080 | Gen: ethay airysay onsssshay-inway isway orkingway\n",
            "Epoch:  97 | Train loss: 0.751 | Val loss: 1.088 | Gen: ethay airysay onssingshay issay orkingway\n",
            "Epoch:  98 | Train loss: 0.742 | Val loss: 1.072 | Gen: ethay airysay onssingshay issay orksay-inestray\n",
            "Epoch:  99 | Train loss: 0.750 | Val loss: 1.080 | Gen: ethay airysay onsssingdray isway orkingway\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airysay onsssingdray isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p2kPGj5DFv7a",
        "colab_type": "code",
        "outputId": "d3a71d28-4801-444d-a84c-011725aab090",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = 'can i have some enjoyable cake and drink'\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_encoder, rnn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tcan i have some enjoyable cake and drink \n",
            "translated:\tancay iway avedray omedsay enonoundershay akentway andway inksay\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7cP7nl5NRJbu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## RNN attention decoder"
      ]
    },
    {
      "metadata": {
        "id": "nKlyfbuPDXDR",
        "colab_type": "code",
        "outputId": "3d2bbab8-20c5-444a-fc28-7502dde19fff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1095
        }
      },
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005, \n",
        "              'lr_decay':0.99,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':20, \n",
        "              'decoder_type': 'rnn_attention', # options: rnn / rnn_attention / transformer\n",
        "              'attention_type': 'scaled_dot',  # options: additive / scaled_dot\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "rnn_attn_encoder, rnn_attn_decoder = train(args)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                            hidden_size: 20                                     \n",
            "                          learning_rate: 0.005                                  \n",
            "                             batch_size: 64                                     \n",
            "                                nepochs: 100                                    \n",
            "                                   cuda: 1                                      \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                           decoder_type: rnn_attention                          \n",
            "                               lr_decay: 0.99                                   \n",
            "                         attention_type: scaled_dot                             \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('payment', 'aymentpay')\n",
            "('ordination', 'ordinationway')\n",
            "('amends', 'amendsway')\n",
            "('principally', 'incipallypray')\n",
            "('anybody', 'anybodyway')\n",
            "Num unique word pairs: 6387\n",
            "Vocabulary: ['EOS', '-', 'SOS', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z']\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.518 | Val loss: 2.052 | Gen: eeday ay ininninnay insssay ininnnay\n",
            "Epoch:   1 | Train loss: 1.904 | Val loss: 1.718 | Gen: etay aray oningingngay iissssay ongningngay\n",
            "Epoch:   2 | Train loss: 1.641 | Val loss: 1.544 | Gen: ety aray onginingingay isssay onensingday\n",
            "Epoch:   3 | Train loss: 1.483 | Val loss: 1.441 | Gen: eshay aray onsingingay istay orringday\n",
            "Epoch:   4 | Train loss: 1.364 | Val loss: 1.355 | Gen: ethsay airray ongingingay isssay orringway\n",
            "Epoch:   5 | Train loss: 1.222 | Val loss: 1.208 | Gen: ethay airray ondionsingway issay ortingway\n",
            "Epoch:   6 | Train loss: 1.108 | Val loss: 1.083 | Gen: ethay airray ondigingingway issay urengay\n",
            "Epoch:   7 | Train loss: 0.986 | Val loss: 1.005 | Gen: eethay airray ondigingway issay oresgay\n",
            "Epoch:   8 | Train loss: 0.864 | Val loss: 0.870 | Gen: etay airay ondiiongingway issay orengway\n",
            "Epoch:   9 | Train loss: 0.798 | Val loss: 0.814 | Gen: epay airay onditingway issay orengway\n",
            "Epoch:  10 | Train loss: 0.707 | Val loss: 0.745 | Gen: epay airay onditingway issay orengay\n",
            "Epoch:  11 | Train loss: 0.667 | Val loss: 0.680 | Gen: epay airay unditingway isisay ortingway\n",
            "Epoch:  12 | Train loss: 0.571 | Val loss: 0.615 | Gen: epay airay onditingway isay orkingway\n",
            "Epoch:  13 | Train loss: 0.532 | Val loss: 0.645 | Gen: epay airway onditinghay isay orkingway\n",
            "Epoch:  14 | Train loss: 0.505 | Val loss: 0.553 | Gen: ecay airway onditiongway isway orkingway\n",
            "Epoch:  15 | Train loss: 0.471 | Val loss: 0.681 | Gen: epay airay onditingcay isway orkingway\n",
            "Epoch:  16 | Train loss: 0.514 | Val loss: 0.516 | Gen: ecay airway onditioncay isway orkingway\n",
            "Epoch:  17 | Train loss: 0.402 | Val loss: 0.456 | Gen: ecay airway onditioncay isway orkingway\n",
            "Epoch:  18 | Train loss: 0.335 | Val loss: 0.455 | Gen: ecay airway onditioncay isway orkingway\n",
            "Epoch:  19 | Train loss: 0.305 | Val loss: 0.442 | Gen: ecay airway onditioncay isway orkingway\n",
            "Epoch:  20 | Train loss: 0.316 | Val loss: 0.404 | Gen: ecay airway onditioncay isway orkingway\n",
            "Epoch:  21 | Train loss: 0.272 | Val loss: 0.336 | Gen: eway airway onditioncay isway orkingway\n",
            "Epoch:  22 | Train loss: 0.328 | Val loss: 0.472 | Gen: ecthay airway onditioncay isway orkingway\n",
            "Epoch:  23 | Train loss: 0.292 | Val loss: 0.326 | Gen: ethay airway onditioncay isway orkingway\n",
            "Epoch:  24 | Train loss: 0.223 | Val loss: 0.321 | Gen: etay airway onditioncay isway orkingway\n",
            "Epoch:  25 | Train loss: 0.204 | Val loss: 0.327 | Gen: ehay airway onditioncay isway orkingway\n",
            "Epoch:  26 | Train loss: 0.208 | Val loss: 0.305 | Gen: etay airway onditioncay isway orkingway\n",
            "Epoch:  27 | Train loss: 0.181 | Val loss: 0.273 | Gen: etay airway onditioncay isway orkingway\n",
            "Epoch:  28 | Train loss: 0.167 | Val loss: 0.268 | Gen: ethay airway onditioncay isway orkingway\n",
            "Epoch:  29 | Train loss: 0.212 | Val loss: 0.324 | Gen: ethay airway onditioncay isway orkingway\n",
            "Epoch:  30 | Train loss: 0.194 | Val loss: 0.286 | Gen: etay airway onditioncay isway orkingway\n",
            "Epoch:  31 | Train loss: 0.168 | Val loss: 0.252 | Gen: etay airway onditiongcay isway orkingway\n",
            "Epoch:  32 | Train loss: 0.145 | Val loss: 0.222 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  33 | Train loss: 0.137 | Val loss: 0.232 | Gen: ethay airway onditioncay isway orkingway\n",
            "Epoch:  34 | Train loss: 0.145 | Val loss: 0.199 | Gen: ethay airway onditioncay isway orkingway\n",
            "Epoch:  35 | Train loss: 0.124 | Val loss: 0.207 | Gen: ethay airway onditioncay isway orkingway\n",
            "Epoch:  36 | Train loss: 0.124 | Val loss: 0.316 | Gen: ethay airway onditionccay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vE-hKCxhF3iR",
        "colab_type": "code",
        "outputId": "e242b646-0300-4e53-bcc9-d592abd90ae2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = 'can i have some enjoyable cake and drink'\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tcan i have some enjoyable cake and drink \n",
            "translated:\tancay iway avehay omesay enjoymesleway akecay andway inkdray\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "X8FaZZUWRpY9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Transformer"
      ]
    },
    {
      "metadata": {
        "id": "Ik5rx9qw9KCg",
        "colab_type": "code",
        "outputId": "a0e01d8e-31e1-46a3-fe2f-382ef715c735",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2187
        }
      },
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005, \n",
        "              'lr_decay':0.99,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':20, \n",
        "              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\n",
        "              'num_transformer_layers': 3,\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "transformer_encoder, transformer_decoder = train(args)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, transformer_encoder, transformer_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                            hidden_size: 20                                     \n",
            "                          learning_rate: 0.005                                  \n",
            "                 num_transformer_layers: 3                                      \n",
            "                             batch_size: 64                                     \n",
            "                                nepochs: 100                                    \n",
            "                                   cuda: 1                                      \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                           decoder_type: transformer                            \n",
            "                               lr_decay: 0.99                                   \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('payment', 'aymentpay')\n",
            "('ordination', 'ordinationway')\n",
            "('amends', 'amendsway')\n",
            "('principally', 'incipallypray')\n",
            "('anybody', 'anybodyway')\n",
            "Num unique word pairs: 6387\n",
            "Vocabulary: ['EOS', '-', 'SOS', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z']\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.737 | Val loss: 1.835 | Gen: erayEOSeay aray onctcacay istaayyy oncay\n",
            "Epoch:   1 | Train loss: 1.632 | Val loss: 1.469 | Gen: ertheray airray ongway issssssssshay orray\n",
            "Epoch:   2 | Train loss: 1.264 | Val loss: 1.212 | Gen: erthererererereratEOSr airway ongcay issspay orray\n",
            "Epoch:   3 | Train loss: 1.009 | Val loss: 0.963 | Gen: ertheday airway ongcay issshisway orray\n",
            "Epoch:   4 | Train loss: 0.842 | Val loss: 0.897 | Gen: erthayy airway onipay iswsayy orkedway\n",
            "Epoch:   5 | Train loss: 0.683 | Val loss: 0.700 | Gen: eway airway onitionioninnnnangii isway orkingway\n",
            "Epoch:   6 | Train loss: 0.578 | Val loss: 0.671 | Gen: etay airway oncay isscadiy orkeray\n",
            "Epoch:   7 | Train loss: 0.490 | Val loss: 0.556 | Gen: ethay airway onctiy isway orkangyayy\n",
            "Epoch:   8 | Train loss: 0.369 | Val loss: 0.572 | Gen: ejay airwy onditioningcay isway orkangway\n",
            "Epoch:   9 | Train loss: 0.356 | Val loss: 0.543 | Gen: ethay airay onditingcay isway orkangaay\n",
            "Epoch:  10 | Train loss: 0.294 | Val loss: 0.413 | Gen: ethay airay onditiongcay isway orkay\n",
            "Epoch:  11 | Train loss: 0.263 | Val loss: 0.460 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  12 | Train loss: 0.229 | Val loss: 0.345 | Gen: ethay airay onditiongcay isway orkaygeay\n",
            "Epoch:  13 | Train loss: 0.205 | Val loss: 0.351 | Gen: ethay airwy onditingcay isway orkingway\n",
            "Epoch:  14 | Train loss: 0.216 | Val loss: 0.334 | Gen: ethay airay onditiongcay isway orkingway\n",
            "Epoch:  15 | Train loss: 0.158 | Val loss: 0.257 | Gen: ethay airay ondititinay isway orkingway\n",
            "Epoch:  16 | Train loss: 0.164 | Val loss: 0.291 | Gen: ethay airwaEOSy onditingcay isway orkingway\n",
            "Epoch:  17 | Train loss: 0.156 | Val loss: 0.229 | Gen: ethayyeey airay onditiningcay isway orkingway\n",
            "Epoch:  18 | Train loss: 0.132 | Val loss: 0.257 | Gen: ethayytay airwaEOSy onditioniongcay isway orkingway\n",
            "Epoch:  19 | Train loss: 0.115 | Val loss: 0.258 | Gen: ethaay airay onditingcay isway orkingway\n",
            "Epoch:  20 | Train loss: 0.150 | Val loss: 0.330 | Gen: etty airay onditingcay isway orkinEOSEOSay\n",
            "Epoch:  21 | Train loss: 0.303 | Val loss: 0.422 | Gen: eay airiy onditininingnayyy isway orkingway\n",
            "Epoch:  22 | Train loss: 0.406 | Val loss: 0.392 | Gen: ethay airwaEOSy onditingcay isway orkragway\n",
            "Epoch:  23 | Train loss: 0.146 | Val loss: 0.217 | Gen: ethay airay onditingcay isway orkingway\n",
            "Epoch:  24 | Train loss: 0.080 | Val loss: 0.196 | Gen: ethay airay onditingcay isway orkingway\n",
            "Epoch:  25 | Train loss: 0.064 | Val loss: 0.196 | Gen: ethay airay onditingcay isway orkingway\n",
            "Epoch:  26 | Train loss: 0.069 | Val loss: 0.209 | Gen: ethay airay onditingcay isway orkingway\n",
            "Epoch:  27 | Train loss: 0.069 | Val loss: 0.176 | Gen: ethay airay onditiny isway orkingway\n",
            "Epoch:  28 | Train loss: 0.049 | Val loss: 0.192 | Gen: ethay airay onditingcay isway orkingway\n",
            "Epoch:  29 | Train loss: 0.056 | Val loss: 0.160 | Gen: ethay airay onditingcay isway orkingway\n",
            "Epoch:  30 | Train loss: 0.059 | Val loss: 0.166 | Gen: ethay airay onditingcay isway orkingway\n",
            "Epoch:  31 | Train loss: 0.045 | Val loss: 0.229 | Gen: ethay airiwy onditinicayy iswayy orkingway\n",
            "Epoch:  32 | Train loss: 0.112 | Val loss: 0.256 | Gen: ethy aiway onditingcay isway orkingway\n",
            "Epoch:  33 | Train loss: 0.094 | Val loss: 0.138 | Gen: ethay airay onditingcayy isswy orkingway\n",
            "Epoch:  34 | Train loss: 0.084 | Val loss: 0.168 | Gen: ethay airay onditingcy isway orkingway\n",
            "Epoch:  35 | Train loss: 0.072 | Val loss: 0.194 | Gen: ethay airiy onditingcay isway orkingwayy\n",
            "Epoch:  36 | Train loss: 0.048 | Val loss: 0.140 | Gen: ethay airay onditingcay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.029 | Val loss: 0.132 | Gen: ethay airay onditingcay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.034 | Val loss: 0.131 | Gen: ethay aiway onditingcay isway orkingsy\n",
            "Epoch:  39 | Train loss: 0.029 | Val loss: 0.122 | Gen: ethay airay onditiningcay isway orkingway\n",
            "Epoch:  40 | Train loss: 0.019 | Val loss: 0.149 | Gen: ethay airay onditingcay isway orkingway\n",
            "Epoch:  41 | Train loss: 0.026 | Val loss: 0.127 | Gen: ethay airay onditingcay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.034 | Val loss: 0.179 | Gen: eey iiray onditiningcay isway orkingway\n",
            "Epoch:  43 | Train loss: 0.269 | Val loss: 0.340 | Gen: egay airay ondingcay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.216 | Val loss: 0.237 | Gen: ethay airay onditininingcay isswy orkingpy\n",
            "Epoch:  45 | Train loss: 0.154 | Val loss: 0.162 | Gen: ethay aiway onditiningcay isway orkingway\n",
            "Epoch:  46 | Train loss: 0.102 | Val loss: 0.162 | Gen: ethay airay onditiningcay issway orkingway\n",
            "Epoch:  47 | Train loss: 0.061 | Val loss: 0.142 | Gen: ethay airay ondititingcay isway orkingway\n",
            "Epoch:  48 | Train loss: 0.091 | Val loss: 0.212 | Gen: ethay array onditioningcay issway orkingwy\n",
            "Epoch:  49 | Train loss: 0.097 | Val loss: 0.219 | Gen: ethay aiway onditiningcay isway orkingway\n",
            "Epoch:  50 | Train loss: 0.087 | Val loss: 0.115 | Gen: ethy airay onditiningcay isway orkingwaEOSy\n",
            "Epoch:  51 | Train loss: 0.058 | Val loss: 0.153 | Gen: ethay airwaEOSwy onditiningcuy isswy orkingway\n",
            "Epoch:  52 | Train loss: 0.044 | Val loss: 0.108 | Gen: ethay airayEOSwy onditinincay isway orkingway\n",
            "Epoch:  53 | Train loss: 0.025 | Val loss: 0.084 | Gen: ethay iiway onditiningcay isswy orkingway\n",
            "Epoch:  54 | Train loss: 0.018 | Val loss: 0.075 | Gen: ethay aiway onditiningcy isway orkingwayy\n",
            "Epoch:  55 | Train loss: 0.011 | Val loss: 0.071 | Gen: ethay aiway onditiningcay isway orkingway\n",
            "Epoch:  56 | Train loss: 0.011 | Val loss: 0.067 | Gen: ethay aiway onditinicayy isway orkingwayy\n",
            "Epoch:  57 | Train loss: 0.012 | Val loss: 0.098 | Gen: ethay aiway onditiongcay isway orkingwayy\n",
            "Epoch:  58 | Train loss: 0.013 | Val loss: 0.077 | Gen: ethay aiway onditiningcay isway orkingway\n",
            "Epoch:  59 | Train loss: 0.008 | Val loss: 0.090 | Gen: ethay aiway onditioningcy isway orkingwayEOSy\n",
            "Epoch:  60 | Train loss: 0.009 | Val loss: 0.084 | Gen: ethay aiway onditiningcay isway orkingwayy\n",
            "Epoch:  61 | Train loss: 0.012 | Val loss: 0.101 | Gen: ethay aiway onditingggcyy isway orkingwayy\n",
            "Epoch:  62 | Train loss: 0.015 | Val loss: 0.103 | Gen: ethay aiway onditioningay isway orkingway\n",
            "Epoch:  63 | Train loss: 0.031 | Val loss: 0.094 | Gen: ethay aiway ondititinicay isway orkingway\n",
            "Epoch:  64 | Train loss: 0.028 | Val loss: 0.118 | Gen: ethay aiway ondititiningcay isway orkingway\n",
            "Epoch:  65 | Train loss: 0.088 | Val loss: 0.444 | Gen: eayy iiray ondioningcayEOSoniiig isway orkingny\n",
            "Epoch:  66 | Train loss: 0.185 | Val loss: 0.173 | Gen: ethethay airsy onditiningcayy isway orkingway\n",
            "Epoch:  67 | Train loss: 0.063 | Val loss: 0.124 | Gen: ethay aiway onditiningcy iay orkingway\n",
            "Epoch:  68 | Train loss: 0.037 | Val loss: 0.126 | Gen: ethay aiway onditiningcay issay orkingway\n",
            "Epoch:  69 | Train loss: 0.024 | Val loss: 0.090 | Gen: ethay airay onditiningcay iay orkingway\n",
            "Epoch:  70 | Train loss: 0.012 | Val loss: 0.096 | Gen: ethay aiway onditiningcay iay orkingway\n",
            "Epoch:  71 | Train loss: 0.009 | Val loss: 0.082 | Gen: ethay aiway onditiningcay iay orkingway\n",
            "Epoch:  72 | Train loss: 0.007 | Val loss: 0.088 | Gen: ethay aiway onditiningcay isway orkingway\n",
            "Epoch:  73 | Train loss: 0.005 | Val loss: 0.076 | Gen: ethay aiway onditiningcay isway orkingway\n",
            "Epoch:  74 | Train loss: 0.004 | Val loss: 0.086 | Gen: ethay aiway onditiningcay isway orkingway\n",
            "Epoch:  75 | Train loss: 0.005 | Val loss: 0.089 | Gen: ethay aiway onditiningcay isway orkingway\n",
            "Epoch:  76 | Train loss: 0.013 | Val loss: 0.113 | Gen: ethay aiway onditiningcay isway orkingway\n",
            "Epoch:  77 | Train loss: 0.032 | Val loss: 0.144 | Gen: ethay airay onditiningcay isway orkingway\n",
            "Epoch:  78 | Train loss: 0.030 | Val loss: 0.194 | Gen: ethay airwy onditiningcay issay orkingway\n",
            "Epoch:  79 | Train loss: 0.060 | Val loss: 0.152 | Gen: ethay airwy onditiningcay isway orkingway\n",
            "Epoch:  80 | Train loss: 0.023 | Val loss: 0.109 | Gen: ethay airwy onditiningcayy iswy orkingway\n",
            "Epoch:  81 | Train loss: 0.017 | Val loss: 0.116 | Gen: ethay aiway onditiningcayy isway orkingway\n",
            "Epoch:  82 | Train loss: 0.016 | Val loss: 0.128 | Gen: ethay aiway onditioiggcay iswy orkingway\n",
            "Epoch:  83 | Train loss: 0.008 | Val loss: 0.092 | Gen: ethaay aiway onditiningcay iswy orkingway\n",
            "Epoch:  84 | Train loss: 0.006 | Val loss: 0.087 | Gen: ethaay aiway onditiningcay iswy orkingway\n",
            "Epoch:  85 | Train loss: 0.004 | Val loss: 0.076 | Gen: ethaay aiway onditioningcay isway orkingway\n",
            "Epoch:  86 | Train loss: 0.005 | Val loss: 0.089 | Gen: eey aiway onditiningcay isway orkingway\n",
            "Epoch:  87 | Train loss: 0.004 | Val loss: 0.085 | Gen: ethaay aiway onditioningcay isway orkingway\n",
            "Epoch:  88 | Train loss: 0.006 | Val loss: 0.111 | Gen: ethay airwy onditiningcay isway orkingway\n",
            "Epoch:  89 | Train loss: 0.009 | Val loss: 0.085 | Gen: ethay aiway onditiningcayy isway orkingway\n",
            "Epoch:  90 | Train loss: 0.009 | Val loss: 0.072 | Gen: ethay aiway onditiningcay isay orkingway\n",
            "Epoch:  91 | Train loss: 0.006 | Val loss: 0.088 | Gen: ethay aiway onditionigncEOSy isway orkingway\n",
            "Epoch:  92 | Train loss: 0.015 | Val loss: 0.117 | Gen: ethay aiway onditiningcay isway orkingway\n",
            "Epoch:  93 | Train loss: 0.010 | Val loss: 0.101 | Gen: ethaay aiway onditiningcay iswy orkingway\n",
            "Epoch:  94 | Train loss: 0.005 | Val loss: 0.094 | Gen: ethay aiway onditioningcay iswy orkingway\n",
            "Epoch:  95 | Train loss: 0.020 | Val loss: 0.115 | Gen: ethay airay onditioningcay iswy orkingway\n",
            "Epoch:  96 | Train loss: 0.030 | Val loss: 0.159 | Gen: ethay iiwy onditioningcay isway orkingway\n",
            "Epoch:  97 | Train loss: 0.088 | Val loss: 0.161 | Gen: ethaey aiway onditinacay isay orkingway\n",
            "Epoch:  98 | Train loss: 0.036 | Val loss: 0.125 | Gen: ethay aiway onditiningcay isy orkingway\n",
            "Epoch:  99 | Train loss: 0.042 | Val loss: 0.124 | Gen: ethay iiway onditiningcay isy orkingway\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay iiway onditiningcay isy orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ULCMHm5ZF7vx",
        "colab_type": "code",
        "outputId": "fe167197-380d-4ca3-b18d-7c5db8c6e6fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = 'can i have some enjoyable cake and drink'\n",
        "translated = translate_sentence(TEST_SENTENCE, transformer_encoder, transformer_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tcan i have some enjoyable cake and drink \n",
            "translated:\tay iway ay - enjoy akecakecakecakecakec ay inkdraay\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qbfZCByITOI6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Attention visualization"
      ]
    },
    {
      "metadata": {
        "id": "itCGMv3FdXsn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TEST_WORD_ATTN = 'aardvark'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xBv4QQuBiU-V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualize RNN attention map"
      ]
    },
    {
      "metadata": {
        "id": "aXvqoQYONMTA",
        "colab_type": "code",
        "outputId": "aea1dc18-cbc5-41c2-f41f-da4753f601fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        }
      },
      "cell_type": "code",
      "source": [
        "visualize_attention(TEST_WORD_ATTN, rnn_attn_encoder, rnn_attn_decoder, None, args)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgsAAAF/CAYAAADHBIqEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'aurdsarkway'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "metadata": {
        "id": "xuOvxfA1NMz3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualize transformer attention maps from all the transformer layers"
      ]
    },
    {
      "metadata": {
        "id": "HSSB4wd8-M7g",
        "colab_type": "code",
        "outputId": "19bec13f-6411-4401-f769-1fb7478b979e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1200
        }
      },
      "cell_type": "code",
      "source": [
        "visualize_attention(TEST_WORD_ATTN, transformer_encoder, transformer_decoder, None, args, )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgEAAAGFCAYAAABkLyAyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgEAAAGFCAYAAABkLyAyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgEAAAGFCAYAAABkLyAyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKYoAAABciiIAAACX+v9gsCMFzwgqEAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ellway-aneredmay'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    }
  ]
}